# Comprehensive-Optimization-Algorithms-Theory-Implementation-Analysis

This repository presents a **full comparative study** and **implementation** of a variety of optimization algorithms ranging from **linear programming (Revised Simplex Method)**, **regularized regression techniques (Ridge, LASSO, Elastic Net)**, to **classical unconstrained nonlinear optimization methods** in both **1D and multidimensional spaces**.

All algorithms are implemented in **Python** with theoretical backgrounds, pseudocode, and performance analysis on benchmark datasets/functions. This work is based on **Math 303 - Linear and Non-linear Programming for Computational Sciences**.

---

## üìÇ Repository Structure

```
Comprehensive-Optimization-Algorithms-Theory-Implementation-Analysis/
‚îÇ
‚îú‚îÄ‚îÄ revised_simplex_lpp/
‚îÇ   ‚îú‚îÄ‚îÄ revised_simplex_lpp_theory.pdf
‚îÇ   ‚îú‚îÄ‚îÄ revised_simplex_lpp_code.ipynb
‚îÇ
‚îú‚îÄ‚îÄ regularization_regression/
‚îÇ   ‚îú‚îÄ‚îÄ ridge_lasso_elasticnet_theory.pdf
‚îÇ   ‚îú‚îÄ‚îÄ ridge_lasso_elasticnet_code.ipynb
‚îÇ
‚îú‚îÄ‚îÄ classical_optimization_methods/
‚îÇ   ‚îú‚îÄ‚îÄ classical_optimization_theory.pdf
‚îÇ   ‚îú‚îÄ‚îÄ classical_optimization_code.ipynb
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

## 1Ô∏è‚É£ Revised Simplex Method for Linear Programming Problems

### **Overview**
The **Revised Simplex Method** is an efficient variant of the Simplex algorithm that:
- Works on large datasets by updating only essential parts of the tableau.
- Focuses on the **basis matrix** \( B \) and its inverse \( B^{-1} \).
- Reduces memory and computational cost compared to the classical Simplex.

---

### **Mathematical Formulation**
**Reduced Cost Calculation:**
\[
c'_N = c_N - c_B \cdot B^{-1} N
\]
Where:
- \( N \) = matrix of non-basic variable columns  
- \( c_N \) = cost coefficients of non-basic variables  
- \( c_B \) = cost coefficients of basic variables  

**Optimality Condition (Maximization)**:
\[
\text{Optimal if} \quad c'_N \geq 0
\]

**Minimum Ratio Test**:
\[
\theta = \min\left(\frac{x_B}{D}\right)
\]
- \( x_B \) = current basic variable values  
- \( D \) = direction vector for entering variable  

---

### **Algorithm**
```
1. Start with a basic feasible solution (BFS) in standard form.
2. Compute B‚Åª¬π.
3. Compute reduced costs for non-basic variables: Zj - Cj = CB¬∑B‚Åª¬π¬∑Pj - Cj
4. If all reduced costs satisfy optimality ‚Üí STOP.
5. Select entering variable (most negative reduced cost for max problems).
6. Compute direction vector: d = B‚Åª¬π¬∑Pj
7. Apply minimum ratio test ‚Üí select leaving variable.
8. Update basis B and compute new B‚Åª¬π.
9. Repeat until optimal solution is found.
```

---

### **Special Cases Covered**
- Regular Simplex
- Two-Phase Simplex
- Degeneracy
- Alternative Optima
- Unbounded Solution
- Infeasible Solution

---

### **References**
- Taha, H. A., *Operations Research: An Introduction*, 10th ed.  
- Boyd, S., & Vandenberghe, L., *Convex Optimization*  
- Rao, S. S., *Engineering Optimization: Theory and Practice*, 5th ed.

---

## 2Ô∏è‚É£ Regularization in Regression: Ridge, LASSO, Elastic Net

### **Overview**
Regularization improves model generalization by adding a penalty to large coefficients.  
This project implements and compares:
- **Ridge Regression** (L2 penalty)
- **LASSO Regression** (L1 penalty)
- **Elastic Net Regression** (Hybrid L1 + L2)

Two datasets used:
- Housing dataset
- Advertising dataset

---

### **Mathematical Models**

**Ridge**:
\[
F(w) = \text{MSE}(w) + \lambda ||w||^2
\]

**LASSO**:
\[
F(w) = \text{MSE}(w) + \lambda ||w||_1
\]

**Elastic Net**:
\[
F(w) = \text{MSE}(w) + \lambda \left( \alpha ||w||_1 + \frac{1 - \alpha}{2} ||w||^2 \right)
\]

---

### **From-Scratch Implementation Steps**
**Ridge Pseudocode:**
```
1. Add bias term to X if needed.
2. Compute regularized matrix: (X·µÄX + ŒªI)
3. Compute weights: w = (X·µÄX + ŒªI)‚Åª¬πX·µÄy
4. Predict: y_pred = X¬∑w
```

**LASSO Pseudocode (Coordinate Descent):**
```
Initialize Œ≤ = 0
Repeat until convergence:
    For each coefficient Œ≤j:
        Compute partial residual rj = y ‚àí XŒ≤ + XjŒ≤j
        sj = Xj·µÄrj
        Œ≤j = sign(sj)¬∑max(|sj| ‚àí Œª, 0) / (Xj·µÄXj)
```

**Elastic Net:**
- Similar to LASSO but with combined L1 and L2 penalties.
- Parameters Œª and Œ± tuned via cross-validation.

---

### **Comparison Table**

| Method     | Feature Selection | Handles Multicollinearity | Extra Parameters | Sparsity |
|------------|-------------------|---------------------------|------------------|----------|
| Ridge      | No                | Yes                       | Œª                | No       |
| LASSO      | Yes               | Partially                 | Œª                | Yes      |
| ElasticNet | Yes               | Yes                       | Œª, Œ±             | Yes      |

---

## 3Ô∏è‚É£ Classical Optimization Methods (1D & Multidimensional)

### **Part 1 ‚Äî 1D Minimization Methods**
- **Fibonacci Method**
- **Golden Section Method**
- **Newton‚Äôs Method**
- **Quasi-Newton Method**
- **Secant Method**

---

### **Part 2 ‚Äî Multidimensional Unconstrained Optimization**
- **Fletcher‚ÄìReeves Conjugate Gradient**
- **Marquardt Method**
- **Quasi-Newton (BFGS)**

---

### **Benchmark Functions**
**Rosenbrock Function:**
\[
f(x_1, x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2
\]
Start: \( X_0 = (-1.2, 1.0) \)

**Powell‚Äôs Quartic Function:**
\[
f(x_1, x_2, x_3, x_4) = (x_1 + 10x_2)^2 + 5(x_3 - x_4)^2 + (x_2 - 2x_3)^4 + 10(x_1 - x_4)^4
\]
Start: \( X_0 = (3.0, -1.0, 0.0, 1.0) \)

---

### **Results ‚Äî Rosenbrock**
| Method           | Iterations | Optimal Solution         | Optimal Value      | CPU Time (s) |
|------------------|-----------:|-------------------------|--------------------|--------------|
| Fletcher-Reeves  | 41         | [1.00000291, 1.00000585] | 8.563√ó10‚Åª¬π¬≤         | 0.018179     |
| Marquardt        | 41         | [0.99999998, 0.99999996] | 4.866√ó10‚Åª¬π‚Å∂         | 0.003407     |
| BFGS             | 18         | [1.0, 1.0]               | 1.010√ó10‚Åª¬π‚Å∏         | 0.007212     |

---

### **Results ‚Äî Powell**
| Method           | Iterations | Optimal Value            | CPU Time (s) |
|------------------|-----------:|--------------------------|--------------|
| Fletcher-Reeves  | 1000       | 9.061√ó10‚Åª¬π‚Å∞               | 0.341768     |
| Marquardt        | 100        | 2.093√ó10                  | 0.004395     |
| BFGS             | 32         | 1.167√ó10‚Åª¬π‚Å∞               | 0.008371     |

---

### **Strengths & Weaknesses Summary**
| Method        | Strengths                                | Weaknesses                         |
|---------------|------------------------------------------|-------------------------------------|
| Fibonacci     | Guaranteed convergence, derivative-free  | Needs Fibonacci precomputation      |
| GoldenSection | Simple, derivative-free                  | Slightly slower than Fibonacci      |
| Newton        | Quadratic convergence                    | Requires Hessian                    |
| Quasi-Newton  | No Hessian, superlinear convergence      | More costly than Newton             |
| Secant        | Derivative-free                          | Can diverge with poor guess         |
| CG (FR)       | Large-scale efficiency                   | Sensitive to line search accuracy   |
| Marquardt     | Robust, blends Newton & descent          | Œª tuning needed                     |
| BFGS          | Fast, robust                             | Matrix ops costly in high dimensions|

---

## ‚öôÔ∏è How to Run

### **Requirements**
- Python 3.8+
- NumPy
- SciPy
- scikit-learn
- Matplotlib

Install dependencies:
```bash
pip install numpy scipy scikit-learn matplotlib
```

### **Running the Notebooks**
```bash
jupyter notebook
```
Open the `.ipynb` files in the corresponding folders.

---

## üìö References
- S. Boyd, L. Vandenberghe, *Convex Optimization*
- S. S. Rao, *Engineering Optimization: Theory and Practice*
- H. A. Taha, *Operations Research: An Introduction*
- T. Hastie, R. Tibshirani, J. Friedman, *The Elements of Statistical Learning*
- Ian Goodfellow, Yoshua Bengio, Aaron Courville, *Deep Learning*
